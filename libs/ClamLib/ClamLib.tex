\documentclass[11pt]{article}
\usepackage{listings}
\setlength{\textwidth}{15.0cm}
\setlength{\oddsidemargin}{0cm}
\setlength{\evensidemargin}{0cm}
\begin{document}
\lstset{language=C++,basicstyle=\tiny}

\title{ClamLib-Turning Artificial Neural Networks into Dynamical Neural Simulations}

\author{Marc de Kamps \and Dave Harrison}
\date{\today}

\maketitle
\begin{abstract}
Despite being criticised for their lack of biological plausibility, Artificial Neural Networks (ANNs) are still the workhorses of cognitive modelling. In contrast to biological networks, in some cases there is a good understanding of how ANNs process information, and in some cases, given how efficient ANNs can be, there is still the expectation that there are networks in the brain which perform similar functions. In recent years a more detailed understanding of the behaviour of populations of neurons has grown, and it suggests that in some cases ANNs may be interpreted as the steady state of populations of spiking neurons.

Another development of the last decade is the maturation of non invasive imaging techniques, which now offer a view of activity inside a brain engaged in cognitive tasks. Increasingly, these techniques offer information on the dynamics of large-scale neuronal networks. In order to compare ANN models with these experimentally observed dynamical networks, it is necessary to extend the ANN models to include neural dynamics.

ClamLib is designed to do this. ClamLib is able to ANN models and to convert them into networks with a biologically plausible neural dynamics. To do this it relies on MIIND, a C++ framework which contains several sophisticated algorithms to model the behaviour of groups of neurons.

ClamLib has three important functions:
\begin{itemize}
\item It converts ANNs into dynamical networks of neural populations
\item It keeps track of the spatial and functional organization of the original ANNs, so that several ANNs can be organized into a hierarchical dynamical network.
\item It offers facilities to record, store, visualise and analyse the simulation data.
\end{itemize}
ClamLib is designed to support the development of the CLAM model, but can easily be adapted to support other models. It has a SWIG generated Python interface, which allows rapid prototyping.
\end{abstract}

\section{Introduction}
A simple example serves best to introduce the basic features of ClamLib.
In Fig. \ref{fig-smallpositive} a small ANN is shown. It is a feedforward network and the activity of the nodes is given by
\begin{equation}
 o = f( \sum_i w_ix_i),
\label{eq-ann}
\end{equation}
where $o$ is the activity of a node, $w_i$ is the weight of its i-th input and $x_i$ is the activity of another node which is connected to the i-th input of this node.
The sigmoid function $f(x)$ is smooth and often given by:
\begin{equation}
f(x) = \frac{1}{1+ e^{-x}}.
\label{eq-sigmoid}
\end{equation} 
In this particular cases, it means that for $x \rightarrow -\infty, f(x) =0$ and $x \rightarrow \infty, f(x) =1$. A natural way to interpret this in neural terms is to say that for high negative input a node is inhibited, whereas for high positive input it is stimulated or excited.

Wilson-Cowan dynamics is a simplified model of the behaviour of a group of spiking neurons. The population firing rate of a group of neurons is the faction of neurons that fires in a short time window, $\Delta t$, divided by $\Delta t$. Wilson-Cowan dynamics is given by:
\begin{equation}
\tau \frac{ dE}{dt} = - E + f(\sum_i w_i E_i)
\label{eq-wilson}
\end{equation}
Here, $E$ is the population firing rate of the group, $E_i$ are the firing rates of other populations, which are connected to the group via weighted connection $i$ (with weight $w_i$). Although the original motivation for this dynamics has been criticised, the dynamics can also be inferred from sophisticated methods for modelling population dynamics and has recently been shown to reproduce neuronal dynamics very reliably in some cases.

Equation \ref{eq-wilson} gives a very direct interpretation for the activation in ANNs as given by equation \ref{eq-ann}: if $\frac{dE}{dt} = $, equation \ref{eq-wilson} reads:
\begin{equation}
e = f(\sum_i w_i E_i), 
\end{equation}
i.e. if the  sigmoids are identical in both equations, the equations are identical as well. The sigmoid in equation \ref{eq-wilson} arises from neuroscience considerations and will not be of the form of equation \ref{eq-sigmoid}, but this is a minor issue. 

This gives a direct interpretation for activation ANNs: they represent steady state activation of neural populations described by Wilson-Cowan dynamics. \footnote{Eq. \ref{eq-wilson} looks suspiciously like equations describing leaky-integrate-and-fire neurons. These describe discontinuous behaviour-the membrane potential is reset after a spike- of individual neurons. It is therefore incorrect to associate this dynamics with population dynamics.}

This suggests a direct possibility for converting ANNs into networks of dynamical simulations. DynamicNetworks are objects that can be instantiated using MIINDs DynamicLib library. DynamicNetworks can be used to simulate networks of populations described by Wilson-Cowan dynamics. The most direct way of associating ANNs with neural dynamics is to generate a Wilson-Cowan dynamics simulation form a  
\begin{figure}[ht]
\begin{lstlisting}[frame=single]
	// Purpose: Generate a DynamicNetwork from a trained SparseLayeredBetwork.
	//			The type of network is determined by the mode
	// Author:  Marc de Kamps
	// Date:    27-09-2006


	// create an input field so that the initial conditions of the network can be set.
	// the rate functions of the network will be read from this pattern
	OrientedPattern<Rate> 
		input_field
		(
			network._net.Dimensions()[0]._nr_x_pixels,
			network._net.Dimensions()[0]._nr_y_pixels,
			network._net.Dimensions()[0]._nr_features
		);

	D_Pattern pat_trained = network._vec_pattern[0].InPat();
    	SpatialConnectionistNet net = network._net;
	for ( Index i = 0; i < pat_trained.Size(); i++ )
		input_field[i] = pat_trained[i];


	if (mode == DIRECT)
		p_dnet->SetDalesLaw(false);

	WilsonCowanParameter param_exc(20e-3,1,1);
	WilsonCowanParameter param_inh(10e-3,1,1);

	auto_ptr<D_AbstractAlgorithm> p_exc;
	auto_ptr<D_AbstractAlgorithm> p_inh;
	
	if ( mode == DIRECT)
	{
		p_exc = auto_ptr<WilsonCowanAlgorithm>(new WilsonCowanAlgorithm(param_exc));
		p_inh = auto_ptr<WilsonCowanAlgorithm>(new WilsonCowanAlgorithm(param_inh));
	}
	else
	{
		p_exc = auto_ptr<SemiSigmoid>(new SemiSigmoid(param_exc));
		p_inh = auto_ptr<SemiSigmoid>(new SemiSigmoid(param_inh));
	}

	AddTNToDN convert;

	convert.Convert
	(
		network,
		input_field,
		p_dnet,
		p_exc.get(),
		p_inh.get()
	);


	return true;
\end{lstlisting}
\caption{Code for converting an Artificial Neural network into a DynamicNetwork simulation, using Wilson-Cowan dynamics.}
\label{fig-smallpositive}
\end{figure}
\end{document}

\section{The anatomy of conversions}
\subsection{Direct conversion}
In general, it is not sufficient to have the only network structure of the ANN available. In a dynamical situation a network needs input and this input is usually related to 
the patterns that were used during training of the ANN. Hence, it makes sense to create an object which represents both the ANN, and the patterns that were used in its 
training. Such an object is the TrainedNet (see Fig. \ref{fig-trained}.

As you can see it has two attributes: {\tt _net} and {\tt _vec_pattern}. The _net attribute is the actual network object. The _vec_pattern object can be used as indicated in 
\ref{fig-smallpositive}: _vec_pattern[0].InPat() is the first input pattern that was used to train the network. (Depending on the TrainingAlgorithm there may also be an 
OutPat() pattern; backpropagation, for example, trains on input-output combinations rather than on single patterns).  Using these training patterns, a DynamicNetwork can 
create special input nodes that represent external inputs. 

Let us assume that the conversion mode is direct. This is the simplest example of a conversion. It is straightforward in the sense that there is a one-to-one mapping between 
nodes of the ANN and the DynamicNetwork. The weights in the DynamicNetwork are the same as the weights between corresponding nodes in the ANN. The main complication is that 
the network needs input to something interesting. Input in a DynamicNetwork is provided by an extra node which runs an algorithm that provides constant input.

\section{Keeping track of conversions}
\subsection{
\subsection{Visualization}
\subsection{Creating hierarchical network structures}
